# the name of the model to use; should be something like
#   gpt2-xl or gpt-neo-2.7B or huggyllama/llama-7b
name: chatglm3-6b

name_or_path: /home/blcuzfy2024/Word-Deed-Consistency-Test/dpo/config/model/chatglm3-6b

# the name of the tokenizer to use; if null, will use the tokenizer from the model
tokenizer_name_or_path: null

# override pre-trained weights (e.g., from SFT); optional
archive: null

# the name of the module class to wrap with FSDP; should be something like
#   e.g. GPT2Block, GPTNeoXLayer, LlamaDecoderLayer, etc.
block_name: GLMBlock

# the dtype for the policy parameters/optimizer state
policy_dtype: float16  # 改为float16节省显存

# the mixed precision dtype if using FSDP; defaults to the same as the policy
fsdp_policy_mp: null

# the dtype for the reference model (which is used for inference only)
reference_dtype: float16

# ===== 添加LoRA配置 =====
lora:
    r: 8
    lora_alpha: 16
    dropout: 0.1
    target_modules: ["query_key_value"]  # 已确认正确
    bias: "none"
    task_type: "CAUSAL_LM"